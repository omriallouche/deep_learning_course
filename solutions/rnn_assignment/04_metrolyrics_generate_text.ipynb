{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrolyrics - Generate text using RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's install PyTorch. We'll use the code examples from the PyTorch repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 159
    },
    "colab_type": "code",
    "id": "elKdxaWPY3BL",
    "outputId": "84825475-635e-41cb-eb07-61ca2b741e1d"
   },
   "outputs": [],
   "source": [
    "!conda install pytorch-cpu -c pytorch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "nhbvGrP6Zo8Y",
    "outputId": "6961b83f-0030-40b1-cb77-8042d8acdcb9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'examples'...\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/ceshine/examples.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if we have a GPU available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "colab_type": "code",
    "id": "POuQdOcIbovZ",
    "outputId": "130ecbea-3c7c-4664-c002-2d22d960c46a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'nvidia-smi' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BuusDyBybqR1"
   },
   "source": [
    "## Train a Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "qPbJvHMirTLH",
    "outputId": "cca58c1e-ba68-465f-f16a-37a3deb38dd6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\ydata\\assignments\\examples\\word_language_model\n"
     ]
    }
   ],
   "source": [
    "%cd examples/word_language_model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 11730
    },
    "colab_type": "code",
    "id": "E7nT91b4bp_F",
    "outputId": "af18007e-32e3-4fe2-ae96-19da9e9789b8"
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "!python -u main.py --data ./data/one_liner --emsize 650 --nhid 650 --dropout 0 --epochs 40 --tied  2>&1 | tee train.log  # Test perplexity of 75.96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first understand the API of the call. The data argument refers to a folder with 3 files - `train.txt`, `valid.txt` and `test.txt`. These files contain the entire data.  \n",
    "In the examples, we see that Wikipedia articles are used. Punctuation marks are separated from words by spaces and so get their own token (and can be generated as well). We also see that words are capitalized in the beginning of sentences. Out-of-Vocabulary words are set as `<unk>`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: \n",
    "The `tied` parameter determines if the weights should be tied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we run this on the full data, let's first try to overfit on a very small dataset.  \n",
    "We'll create files with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When generating words, we again pass link to the data - try to think why!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the number of words to generate, and importantly the temperature. The temperature gives us control on the divesity of the generated words. A temperature of 1 selects words according to their softmax probability. When the temperature is low, the top word will always be selected. When the temperature is higher, words with lower probability might still be selected, at a higher probability. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try first with a temperature of 1e-3 (the smallest possible):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python generate.py --data ./data/one_liner --words 50 --temperature 1e-3\n",
    "!type generated.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now with a temperature of 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Generated 0/50 words\n",
      "trigger against the is killed head <eos> a and head just against the trigger man mama killed mama he mama\n",
      "<eos> put just trigger a now dead dead pulled i is he head killed just i he gun i man\n",
      "just his <eos> mama mama <eos> head i and put \n"
     ]
    }
   ],
   "source": [
    "!python generate.py --data ./data/one_liner --words 50 --temperature 1\n",
    "!type generated.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and with a very high temperature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python generate.py --data ./data/one_liner --words 50 --temperature 1000\n",
    "!type generated.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nawq6N3ufi9e"
   },
   "source": [
    "### Download the trained model\n",
    "If we're on Colab, let's download the created model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OEIN3zlvfl5Y"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.download('model.pt')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "01_Training.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
