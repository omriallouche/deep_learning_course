{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding - Home Assigment\n",
    "## Dr. Omri Allouche 2018. YData Deep Learning Course\n",
    "\n",
    "[Open in Google Colab](https://colab.research.google.com/github/omriallouche/deep_learning_course/blob/master/DL_word_embedding_assignment.ipynb)\n",
    "    \n",
    "    \n",
    "In this exercise, you'll use word vectors trained on a corpus of 380,000 lyrics of songs from MetroLyrics (https://www.kaggle.com/gyani95/380000-lyrics-from-metrolyrics).  \n",
    "The dataset contains these fields for each song, in CSV format:\n",
    "1. index\n",
    "1. song\n",
    "1. year\n",
    "1. artist\n",
    "1. genre\n",
    "1. lyrics\n",
    "\n",
    "Before doing this exercise, we recommend that you go over the \"Bag of words meets bag of popcorn\" tutorial (https://www.kaggle.com/c/word2vec-nlp-tutorial)\n",
    "\n",
    "Other recommended resources:\n",
    "- https://rare-technologies.com/word2vec-tutorial/\n",
    "- https://www.kaggle.com/pierremegret/gensim-word2vec-tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install needed packages\n",
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import needed packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import operator\n",
    "# import multiprocessing\n",
    "import logging\n",
    "# from tqdm import tqdm\n",
    "import re\n",
    "import string\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    " \n",
    "import seaborn as sns\n",
    "# sns.set_style(\"darkgrid\")\n",
    "\n",
    "from collections import Counter\n",
    "# from scipy.spatial import distance\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "from spacy.util import minibatch, compounding\n",
    "from spacy.tokenizer import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_WORDS = set(stopwords.words('english'))\n",
    "PUNCT = dict.fromkeys(map(ord, string.punctuation))\n",
    "CPUS = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('lyrics.csv', index_col=0)\n",
    "print('df.shape:', df.shape)\n",
    "print('df.columns:', df.columns)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis\n",
    "Let's examine the data a bit first. We can print the numbers, or plot a bar chart of the counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('genre')['lyrics'].count().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(data=df, y='genre', order=df.groupby('genre')['lyrics'].count().sort_values(ascending=False).index, orient='h')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there are a lot of Rock songs, and very few Indie and Folk songs. How can this class imbalance affect our classifier performance?\n",
    "\n",
    "This class imbalance might affect our algorithms if we're using only the most common words, since these words will be biased by their prevalance in Rock and Pop songs.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also see songs classified as \"Not Available\" or \"Other\". We should probably remove these from our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[ ~df.genre.isin(['Not Available', 'Other'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's examine the distribution of the length of songs. We expect most songs to be around 3-5 minutes, and therefore have a certain length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['num_chars'] = df['lyrics'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['num_chars'].hist(bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a large pick at roughly 1000 characters, but a long tail of long documents, and quite a few documents that are very short. Let's check the CDF (Comulative Distribution Function):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import percentileofscore, scoreatpercentile\n",
    "from numpy import sort, arange, nanpercentile, diff\n",
    "\n",
    "def get_cdf(data, ignore_nan=False):\n",
    "    if ignore_nan:\n",
    "        data = data[ ~np.isnan(data) ]\n",
    "    values = sort(data)\n",
    "    percentiles = arange(len(values))/float(len(values))\n",
    "    return values, percentiles\n",
    "\n",
    "def plot_cdf(data, **kwargs):\n",
    "    ignore_nan = kwargs.pop('ignore_nan', False)\n",
    "    ax = kwargs.pop('ax', plt.gca())\n",
    "    values, percentiles = get_cdf(data, ignore_nan=ignore_nan)\n",
    "    ax.plot(values, percentiles, **kwargs )\n",
    "    return values, percentiles\n",
    "\n",
    "plot_cdf(df['num_chars']);\n",
    "plt.xlim((0, 3000))\n",
    "plt.xlabel('Number of Characters'); plt.ylabel('Proportion'); plt.title('CDF - Number of Characters');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We indeed see that some documents are very short (remember this is the number of characters, including spaces, punctuation marks and new line characters). We will examine the short and long documents later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's examine the length of songs by genre, using a box plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=df, y='genre', x='num_chars')\n",
    "plt.xlim(0, 4000);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that Hip-Hop songs tend to be much longer than the others, but don't see very large differences between other genres. We also note that there are many songs that are very long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.violinplot(data=df, y='genre', x='num_chars')\n",
    "plt.xlim(0, 4000);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that Electronic and Pop songs have large variability in length compared with Jazz and Country."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's examine the longest documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values('num_chars').tail(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine a few of the records with many characters. We can see that these aren't actually songs - they are, for example, interviews or full albums. We don't expect our classifier to classify these correctly or learn from them, and better remove them from our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( df.loc[255126].lyrics )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print( df.loc[230543].lyrics )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print( df.query('num_chars>4000 & num_chars<4500').iloc[2]['lyrics'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other end, let's examine a few of the short songs in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values('num_chars').head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that many songs have only special characters and no actual lyrics. We also see comments in [] blocks. We'd like to make sure we remove these from our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[ df['num_chars']>200 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we look at the number of words and unique number of words for each genre - NOT IMPORTANT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['num_words'] = df['lyrics'].apply(lambda x: len(x.replace('\\n', ' ').split()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['num_unique_words'] = df['lyrics'].apply(lambda x: len( set( x.replace('\\n', ' ').lower().split()) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['average_word_length'] = df['num_chars'] / df['num_words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.boxplot(data=df, y='genre', x='average_word_length')\n",
    "plt.xlim(0, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=df, y='genre', x='num_unique_words')\n",
    "plt.xlim(0, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for genre in df.genre.unique():\n",
    "    d = df[ df.genre==genre ]\n",
    "    sns.regplot(d['num_chars'], y=d['num_unique_words'], x_bins=30, label=genre)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample data \n",
    "In order to explore efficiently, we don't need all of the data we have. Let's create a subset of it, making sure the number of songs from each category generally matches. \n",
    "\n",
    "We first focus only on the most common genres - Rock, Pop, Hip-Hop, Metal and Country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[ df.genre.isin(['Rock', 'Pop', 'Hip-Hop', 'Metal', 'Country'])]\n",
    "genre_counts = df['genre'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [ 1/genre_counts.loc[v] for v in df.genre.values ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = df.sample(n=50000, random_state=10, weights=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.genre.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train word vectors\n",
    "Train word vectors using the Skipgram Word2vec algorithm and the gensim package.\n",
    "Make sure you perform the following:\n",
    "- Tokenize words\n",
    "- Lowercase all words\n",
    "- Remove punctuation marks\n",
    "- Remove rare words\n",
    "- Remove stopwords\n",
    "\n",
    "Use 300 as the dimension of the word vectors. Try different context sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_tokenize(text):\n",
    "    text = re.sub(\"[^a-zA-Z]\",\" \", text)\n",
    "    text = re.sub(\"[\\[*\\]]\",\" \", text)\n",
    "    text = text.translate(PUNCT)\n",
    "    return [word.lower() for word in nltk.word_tokenize(text) if word not in STOP_WORDS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( df.iloc[0].lyrics )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" \".join( nltk_tokenize( df.iloc[0].lyrics ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sent'] = df['lyrics'].apply(nltk_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find the most common and rare words. We can count each genre separately, and then easily combine the counts to get the backgroun distribution. We need to remember that the number of songs in each genre is different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['num_words'] = df.sent.apply(lambda x: len(x))\n",
    "df = df[ df.num_words > 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's train word vectors on our new corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "w2v = gensim.models.Word2Vec(df.sent, sg=1, min_count=4, size=50, workers=CPUS*2-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review most similar words\n",
    "Get initial evaluation of the word vectors by analyzing the most similar words for a few interesting words in the text. \n",
    "\n",
    "Choose words yourself, and find the most similar words to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_check = ['love', 'hate', 'lonely', 'heartache', 'success', 'guitar', 'god', 'beer', 'gun', 'police']\n",
    "for word in words_to_check:\n",
    "    print(word, ' -> ', ['{} ({:.2f}), '.format(tup[0], tup[1]) for tup in w2v.wv.similar_by_word(word, topn=5)])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "w2v = gensim.models.Word2Vec(df.sent, sg=1, min_count=5, size=300, workers=CPUS*2-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_check = ['love', 'hate', 'lonely', 'heartache', 'success', 'guitar', 'god', 'beer', 'gun', 'police']\n",
    "for word in words_to_check:\n",
    "    print(word, ' -> ', ['{} ({:.2f}), '.format(tup[0], tup[1]) for tup in w2v.wv.similar_by_word(word, topn=5)])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Vectors Algebra\n",
    "We've seen in class examples of algebraic games on the word vectors (e.g. man - woman + king = queen ). \n",
    "\n",
    "Try a few vector algebra terms, and evaluate how well they work. Try to use the Cosine distance and compare it to the Euclidean distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar_cosmul(positive=['woman', 'king'], negative=['man'])[0]\n",
    "model.wv.most_similar(positive=['woman', 'king'], negative=['man'])[0]\n",
    "model.wv.most_similar_cosmul(positive=['girl', 'brother'], negative=['boy'])[0]\n",
    "model.wv.most_similar_cosmul(positive=[\"man\",\"daughter\"],negative=[\"woman\"])\n",
    "model.wv.most_similar(positive=['king', 'woman'], negative=['man'])\n",
    "model.wv.most_similar(positive=['mother', 'he'], negative=['father'])\n",
    "model.wv.most_similar(positive=['strong', 'small'], negative=['weak'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n",
    "Estimate sentiment of words using word vectors.  \n",
    "In this section, we'll use the SemEval-2015 English Twitter Sentiment Lexicon.  \n",
    "The lexicon was used as an official test set in the SemEval-2015 shared Task #10: Subtask E, and contains a polarity score for words in range -1 (negative) to 1 (positive) - http://saifmohammad.com/WebPages/SCL.html#OPP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a classifier for the sentiment of a word given its word vector. Split the data to a train and test sets, and report the model performance on both sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by downloading the data and extracting it. We will create a dictionary with the keys are the words and the values are their sentiment scores.  \n",
    "Note that the sentiment dataset contains terms of multiple words and hashtags - we will remove these from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://saifmohammad.com/WebDocs/lexiconstoreleaseonsclpage/SemEval2015-English-Twitter-Lexicon.zip\n",
    "!unzip SemEval2015-English-Twitter-Lexicon.zip\n",
    "!head SemEval2015-English-Twitter-Lexicon/SemEval2015-English-Twitter-Lexicon.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://nlp.stanford.edu/data/glove.twitter.27B.zip\n",
    "!unzip glove.twitter.27B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://radimrehurek.com/gensim/scripts/glove2word2vec.html\n",
    "if False:\n",
    "    tmp_file = get_tmpfile(\"w2v.twitter.27B.100d.txt\")\n",
    "    glove2word2vec('glove.twitter.27B.100d.txt', tmp_file)\n",
    "    model = KeyedVectors.load_word2vec_format(tmp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('SemEval2015-English-Twitter-Lexicon.txt', 'r') as f:\n",
    "    scores, twitter_sentiment_words = zip(*[line.strip().split() for line in f.readlines()])\n",
    "\n",
    "twitter_sentiment_words = [w[1:] if w.startswith('#') else w for w in twitter_sentiment_words]\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for i in range(len(scores)):\n",
    "    if twitter_sentiment_words[i] in model:\n",
    "        X.append(model[twitter_sentiment_words[i]])\n",
    "        y.append(scores[i])\n",
    "\n",
    "X = np.array(X, dtype='float')\n",
    "y = np.array(y, dtype='float')\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "  train_test_split(X, y, test_size=0.1, random_state=0) # We used 10,000 songs from each category - 1,000 songs in the test set seem like a lot. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's build a regressor to predict the sentiment of a word from its word vector. We use the word vectors trained on our corpus of song lyrics.  \n",
    "Let's try a few different algorithms, and choose the one with the smallest MSE on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "models = [\n",
    "    LinearRegression(),\n",
    "    MLPRegressor(max_iter=1000, tol=1e-5, hidden_layer_sizes=(300,200,100)),\n",
    "    RandomForestRegressor(),\n",
    "    GradientBoostingRegressor(),\n",
    "    SVR()\n",
    "]\n",
    "score = {}\n",
    "for model in models:\n",
    "    model_name = str(model).split('(')[0]\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    score[model_name] = metrics.mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use your trained model from the previous question to predict the sentiment score of words in the lyrics corpus that are not part of the original sentiment dataset. Review the words with the highest positive and negative sentiment. Do the results make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 0\n",
    "test_words = []\n",
    "for w, _ in freqs_bg.most_common(10000):\n",
    "    if w not in twitter_sentiment_words and w in w2v:\n",
    "        test_words.append(w)\n",
    "        num_words += 1\n",
    "    if num_words == 1000:\n",
    "        break\n",
    "\n",
    "similar_words_features = np.array([w2v[w] for w in test_words], 'float')\n",
    "sentiment_scores = model.predict(similar_words_features)\n",
    "\n",
    "df_predicted_sentiment = pd.Series(sentiment_scores, index=test_words)\n",
    "\n",
    "print(' --- top 20 negative sentiment score --')\n",
    "print( df_predicted_sentiment.sort_values().head(20) )\n",
    "\n",
    "print(' --- top 20 positive sentiment score --')\n",
    "print( df_predicted_sentiment.sort_values().tail(20) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Word Vectors\n",
    "In this section, you'll plot words on a 2D grid based on their inner similarity. We'll use the tSNE transformation to reduce dimensions from 300 to 2. You can get sample code from https://www.kaggle.com/pierremegret/gensim-word2vec-tutorial or other tutorials online.\n",
    "\n",
    "Perform the following:\n",
    "- Keep only the 3,000 most frequent words (after removing stopwords)\n",
    "- For this list, compute for each word its relative abundance in each of the genres\n",
    "- Compute the ratio between the proportion of each word in each genre and the proportion of the word in the entire corpus (the background distribution)\n",
    "- Pick the top 50 words for each genre. These words give good indication for that genre. Join the words from all genres into a single list of top significant words. \n",
    "- Compute tSNE transformation to 2D for all words, based on their word vectors\n",
    "- Plot the list of the top significant words in 2D. Next to each word output its text. The color of each point should indicate the genre for which it is most significant.\n",
    "\n",
    "You might prefer to use a different number of points or a slightly different methodology for improved results.  \n",
    "Analyze the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "\n",
    "freqs = {}\n",
    "for i,r in tqdm(df.iterrows()):\n",
    "    if r.genre not in freqs:\n",
    "        freqs[ r.genre ] = Counter()\n",
    "    freqs[ r.genre ].update(r.sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs_bg = Counter()\n",
    "[freqs_bg.update(v) for v in freqs.values()];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the most common words for each genre. Remember, here we only consider word occurrences. A word might be common in all genres. We will later examine words that are common in a specific genre more than the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs_bg.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = {genre: [v[0] for v in counts.most_common(20)] for genre, counts in freqs.items()}\n",
    "pd.DataFrame(_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you spot the genre with the least amount of \"love\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the most common words for each genre, we will search for words that are both common and have a large ratio between their relative frequency in a genre and their relative frequency in the background.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_counts = {genre: sum(counts.values()) for genre, counts in freqs.items()}\n",
    "total_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_counts_bg = sum(freqs_bg.values())\n",
    "word_freq = pd.DataFrame({w: freqs_bg[w] / total_counts_bg for w, v in freqs_bg.most_common(2000)}, index=['bg']).T\n",
    "word_freq.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for genre in total_counts.keys():\n",
    "    word_freq[genre] = [ freqs[genre].get(w, 0)/total_counts[genre]/r['bg'] for w,r in word_freq.iterrows() ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres = df.genre.unique()\n",
    "for genre in genres:\n",
    "    print( genre )\n",
    "    print( \", \".join(word_freq.sort_values(genre, ascending=False).index[:50].values ) )\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "words = list(set(list(itertools.chain.from_iterable([word_freq.sort_values(genre, ascending=False).index[:50].values for genre in genres]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "X = [w2v[w] for w in words]\n",
    "X_embedded = TSNE(n_components=2).fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_embedded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['r', 'g', 'b', 'k', 'y']\n",
    "c = [colors[list(genres).index(word_freq.loc[w].argmax())] for w in words]\n",
    "word_genre = np.array( [word_freq.loc[w].argmax() for w in words] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,12))\n",
    "\n",
    "x = X_embedded[:,0];\n",
    "y = X_embedded[:,1];\n",
    "\n",
    "for i,genre in enumerate(genres):\n",
    "    ids = np.where( word_genre==genre )[0]\n",
    "    plt.plot(x[ids], y[ids], '.', color=colors[i], label=genre)\n",
    "for i, word in enumerate(words):\n",
    "    plt.annotate(word, alpha=0.5, xy=(x[i], y[i]), xytext=(5, 2),\n",
    "                 textcoords='offset points', ha='right', va='bottom', size=12)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification\n",
    "In this section, you'll build a text classifier, determining the genre of a song based on its lyrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text classification using Bag-of-Words\n",
    "Build a Naive Bayes classifier based on the bag of Words.  \n",
    "You will need to divide your dataset into a train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "  train_test_split(df.sent, df.genre, test_size=0.1, random_state=0) # We used 10,000 songs from each category - 1,000 songs in the test set seem like a lot. \n",
    "\n",
    "X_train_counts = count_vect.fit_transform([\" \".join(sent) for sent in X_train])\n",
    "X_test_counts = count_vect.transform([\" \".join(sent) for sent in X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "model = MultinomialNB(alpha = 1)\n",
    "model.fit(X_train_counts, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "y_pred = model.predict(X_train_counts)\n",
    "cm = confusion_matrix(y_train, y_pred)\n",
    "print( pd.DataFrame(cm, index=genres, columns=genres) )\n",
    "\n",
    "y_pred = model.predict(X_test_counts)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print( pd.DataFrame(cm, index=genres, columns=genres) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the classification report - precision, recall, f1 for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( classification_report(y_test, y_pred ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.heatmap( confusion_matrix(y_test, y_pred) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text classification using Word Vectors\n",
    "#### Average word vectors\n",
    "Do the same, using a classifier that averages the word vectors of words in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vec = np.array([sum(w2v[w] for w in sent if w in w2v) for sent in X_train])\n",
    "X_test_vec = np.array([sum(w2v[w] for w in sent if w in w2v) for sent in X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_vec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "y_pred = model.predict(X_train_vec)\n",
    "cm = confusion_matrix(y_train, y_pred)\n",
    "print( pd.DataFrame(cm, index=genres, columns=genres) )\n",
    "\n",
    "y_pred = model.predict(X_test_vec)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print( pd.DataFrame(cm, index=genres, columns=genres) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TfIdf Weighting\n",
    "Do the same, using a classifier that averages the word vectors of words in the document, weighting each word by its TfIdf.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(smooth_idf=True, sublinear_tf=False, norm=None, analyzer='word')\n",
    "X_train_tfidf = tfidf_vect.fit_transform([\" \".join(sent) for sent in X_train])\n",
    "X_test_tfidf = tfidf_vect.transform([\" \".join(sent) for sent in X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vec = np.array([sum(w2v[w] for w in sent if w in w2v) for sent in X_train])\n",
    "X_test_vec = np.array([sum(w2v[w] for w in sent if w in w2v) for sent in X_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text classification using ConvNet\n",
    "Do the same, using a ConvNet.  \n",
    "The ConvNet should get as input a 2D matrix where each column is an embedding vector of a single word, and words are in order. Use zero padding so that all matrices have a similar length.  \n",
    "Some songs might be very long. Trim them so you keep a maximum of 128 words (after cleaning stop words and rare words).  \n",
    "Initialize the embedding layer using the word vectors that you've trained before, but allow them to change during training.  \n",
    "\n",
    "Extra: Try training the ConvNet with 2 slight modifications:\n",
    "1. freezing the the weights trained using Word2vec (preventing it from updating)\n",
    "1. random initialization of the embedding layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are encouraged to try this question on your own.  \n",
    "\n",
    "You might prefer to get ideas from the paper \"Convolutional Neural Networks for Sentence Classification\" (Kim 2014, [link](https://arxiv.org/abs/1408.5882)).\n",
    "\n",
    "There are several implementations of the paper code in PyTorch online (see for example [this repo](https://github.com/prakashpandey9/Text-Classification-Pytorch) for a PyTorch implementation of CNN and other architectures for text classification). If you get stuck, they might provide you with a reference for your own code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
